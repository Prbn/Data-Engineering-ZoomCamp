## 1. Why Docker?
Docker allows us to package software into **containers**, which are isolated environments containing everything needed to run an application. 

### Benefits of Docker:
- **Reproducibility**: Identical environments on any system.
- **Isolation**: Avoid dependency conflicts between applications.
- **Portability**: Containers can run on any machine with Docker installed.

### Docker in Data Engineering:
- **Local Experiments**: Quickly set up environments for testing.
- **Integration Testing**: Validate data pipelines with databases.
- **Cloud Deployment**: Run containerized applications on AWS, Google Cloud, or Kubernetes.

---

## 2. What Are Data Pipelines?
A **data pipeline** is a series of processes that ingest data, transform it, and output it to another system.

### Example:
- **Input**: CSV files containing raw data.
- **Transformation**: Python scripts clean and process the data.
- **Output**: A PostgreSQL table or another destination.

Data pipelines often involve multiple steps, chained together to process data efficiently.

---

## 3. Setting Up Docker

### Install Docker
Follow the [Docker Installation Guide](https://docs.docker.com/get-docker/) for your operating system (Windows, macOS, Linux).

### Verify Installation:
Run the following command:
```bash
docker run hello-world
```
This downloads and runs a test container. If successful, you’ll see a message confirming Docker is working.

---

## 4. Creating a Data Pipeline with Docker

### Step 1: Write a Python Script
Create `pipeline.py` for processing data:
```python
import pandas as pd
import sys

if __name__ == "__main__":
    # Accept a command-line argument for the processing day
    day = sys.argv[1] if len(sys.argv) > 1 else "2021-01-01"
    print(f"Processing data for {day}")

    # Example: Simulate processing
    data = pd.DataFrame({"column": [1, 2, 3]})
    print(data)

    print("Processing complete.")
```

### Step 2: Define a Dockerfile
```Dockerfile
FROM python:3.12.8

# Install dependencies
RUN pip install pandas

# Set working directory
WORKDIR /app

# Copy pipeline script
COPY pipeline.py /app

# Set entry point to run the script
ENTRYPOINT ["python", "pipeline.py"]
```

### Step 3: Build and Run the Pipeline
1. Build the Docker image:
   ```bash
   docker build -t python-pipeline .
   ```
   detailed breakdown of its components:
	1.	docker build:
    This initiates the build process to create a Docker image.
	2.	-t python-pipeline:
    The -t (or --tag) flag is used to name the image. Here, the image is tagged as python-pipeline. You can later use this name to refer to the image, for example, when running a container from it using docker run.
	3.	.:
    The dot specifies the build context. It tells Docker to look for the Dockerfile and any required files in the current directory. The Dockerfile contains the instructions on how to build the image, such as what base image to use, what dependencies to install, and how to configure the environment.
2. Run the pipeline for a specific date:
   ```bash
   docker run python-pipeline 2021-01-15
   ```

    Detailed breakdown of Docker Command:
	1. docker run: Creates and starts a new container from the specified image.
	2. python-pipeline: The name of the Docker image built earlier using docker build -t python-pipeline ..
	3. 2021-01-15: This is passed as an argument (sys.argv[1]) to the Python script inside the container.

    What Happens Inside the Container:
    1. The CMD or ENTRYPOINT specified in the Dockerfile is executed
    2. The 2021-01-15 argument is passed to the script, which picks it up as sys.argv[1] in the Python code.

    The script processes the date as follows:
    ```text
    Processing data for 2021-01-15
       column
    0       1
    1       2
    2       3
    Processing complete.
    ```


---

## 5. Running PostgreSQL with Docker

### Step 1: Pull the PostgreSQL Image
```bash
docker pull postgres:13
```
Command Breakdown:
1. docker pull:
    - This tells Docker to fetch an image from a container registry (by default, Docker Hub).
2. postgres:13:
    - postgres: The name of the image, in this case, the official PostgreSQL image.
	- :13: The specific tag for PostgreSQL version 13. If no tag is specified (e.g., just postgres), Docker defaults to the latest tag.

What Happens When You Run This Command
1.	Docker connects to the Docker Hub registry.
2.	It fetches the PostgreSQL image with the specified tag (13).
3.	The image is downloaded and stored locally in your Docker image repository.

### Step 2: Run PostgreSQL in a Container
```bash
docker run -it \
  --name postgres \
  -e POSTGRES_USER=postgres \
  -e POSTGRES_PASSWORD=postgres \
  -e POSTGRES_DB=ny_taxi \
  -p 5432:5432 \
  -v /path/to/data:/var/lib/postgresql/data \
  postgres:13
```
Command Breakdown:
1. docker run: This command creates and starts a new container.
2.	-it: Runs the container in interactive mode, allowing you to interact with the container via the terminal.
3. --name postgres: Assigns the name postgres to the container. You can use this name to reference the container later (e.g., docker exec postgres).
4. Environment Variables (-e):
- -e POSTGRES_USER=postgres: Sets the PostgreSQL username to postgres.
- -e POSTGRES_PASSWORD=postgres: Sets the password for the PostgreSQL user postgres.
- -e POSTGRES_DB=ny_taxi: Creates a default database named ny_taxi in the PostgreSQL instance.
5. Port Mapping (-p):
- `-p 5432:5432`: Maps the container’s PostgreSQL port (5432) to the host’s port (5432), making the database accessible from outside the container.
6. Volume (-v): This flag mounts a volume, linking a directory on your host machine to a directory inside the container.
- /path/to/data:
- Replace this with the absolute path on your host machine where you want to store PostgreSQL data persistently.
7. postgres:13: Specifies the PostgreSQL 13 image to use. If the image is not already downloaded, Docker will pull it from Docker Hub.


What Happens When You Run This Command:
1.	A PostgreSQL container named postgres is created and started.
2.	PostgreSQL is configured with:
- Username: postgres
- Password: postgres
- Default database: ny_taxi
3.	The database service listens on port 5432 on your host machine, allowing you to connect to it via tools like psql or database clients.
4. All database files will now be stored in /path/to/data on your host machine.
- If the container is stopped or removed, the database data will remain available in this directory.
- You can start another PostgreSQL container and point it to the same directory to restore the database.


---